{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nibin101/intelligent_quiz/blob/main/intelligent_quiz_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3yWWtQ8cC4i"
      },
      "source": [
        "# STEP 0: Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR8CG3W7AdEz",
        "outputId": "79372584-0a07-4527-9130-8331cd42ea06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.5/152.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.6/785.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU google-generativeai==0.8.5 google-ai-generativelanguage==0.6.15\\\n",
        "langgraph langchain langchain-google-genai openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvoFPl11cFhj"
      },
      "source": [
        "# STEP 1: Imports and secure API key input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QXaYSMSSCx9j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusINPuVcLXp"
      },
      "source": [
        "# STEP 2: Initialize Gemini 1.5 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xxCohNVcF7PB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f1d753-aec8-444c-9907-1a1f4f4cd41c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the Gemini api key··········\n"
          ]
        }
      ],
      "source": [
        "os.environ['GOOGLE_API_KEY']=getpass.getpass(\"Enter the Gemini api key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28u20SxcIiH"
      },
      "source": [
        "\n",
        "# Secure Gemini API Key input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WnrEGf-yHM9Y"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o07zbkLWcOVW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uNGK9lY7INF1"
      },
      "outputs": [],
      "source": [
        "def get_subject_input(state: dict) -> dict:\n",
        "    \"\"\"Gets the subject input from the user.\"\"\"\n",
        "    subject = input(\"What subject do you want to learn about (physics, chemistry, or maths)? \")\n",
        "    state[\"subject\"] = subject.lower()\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KIzyYPVcRVm"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBIFZ18Pjqd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5ab0l3bHmpbG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def generate_mcqs(state: dict) -> dict:\n",
        "    \"\"\"Generates 5 MCQ questions on the given subject and stores them in the state.\"\"\"\n",
        "    subject = state[\"subject\"]\n",
        "    prompt = (\n",
        "        f\"Generate 5 multiple-choice questions about {subject}. \"\n",
        "        \"Each question should have 4 options (A, B, C, D) and indicate the correct answer. \"\n",
        "        \"Respond in JSON format as a list of objects, where each object has keys 'question', 'options', and 'answer'.\"\n",
        "        \"Do not include any ''' formatting\"\n",
        "    )\n",
        "    response = llm.invoke([HumanMessage(content=prompt)])\n",
        "    print(response.content)\n",
        "    try:\n",
        "        # Attempt to parse the content directly first\n",
        "        questions_json = json.loads(response.content)\n",
        "        state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON response from the language model.\")\n",
        "        print(e)\n",
        "        # If direct parsing fails, try stripping potential markdown formatting\n",
        "        try:\n",
        "            cleaned_response = response.content.strip().replace(\"```json\\n\", \"\").replace(\"```\", \"\")\n",
        "            questions_json = json.loads(cleaned_response)\n",
        "            state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "        except json.JSONDecodeError as e_cleaned:\n",
        "            print(\"Error decoding JSON response after cleaning.\")\n",
        "            print(e_cleaned)\n",
        "            state[\"questions\"] = []\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kESmxbnicWpS"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d90qeH3nbKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90ee11f9-8409-40e4-80e7-4b8315877598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What subject do you want to learn about and be quizzed on? maths\n",
            "Error decoding JSON response from the language model.\n",
            "Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "Here are the questions:\n",
            "\n",
            "Question 1: What is the value of 3 + 4?\n",
            "A. 12\n",
            "B. 25\n",
            "C. 36\n",
            "D. 49\n",
            "Your answer (enter the option letter, e.g., A): 69\n",
            "Incorrect. The correct answer is B.\n",
            "\n",
            "Question 2: Solve for x: 2x + 5 = 11\n",
            "A. 3\n",
            "B. 6\n",
            "C. 8\n",
            "D. 16\n",
            "Your answer (enter the option letter, e.g., A): A\n",
            "Correct!\n",
            "\n",
            "Question 3: What is the area of a rectangle with length 8cm and width 5cm?\n",
            "A. 13cm\n",
            "B. 26cm\n",
            "C. 40cm\n",
            "D. 80cm\n",
            "Your answer (enter the option letter, e.g., A): C\n",
            "Correct!\n",
            "\n",
            "Question 4: What is the next number in the sequence: 2, 4, 6, 8, ...?\n",
            "A. 9\n",
            "B. 10\n",
            "C. 12\n",
            "D. 16\n",
            "Your answer (enter the option letter, e.g., A): D\n",
            "Incorrect. The correct answer is B.\n",
            "\n",
            "Question 5: What is the value of 15% of 200?\n",
            "A. 10\n",
            "B. 20\n",
            "C. 30\n",
            "D. 150\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def get_subject_input(state: dict) -> dict:\n",
        "    \"\"\"Gets the subject input from the user.\"\"\"\n",
        "    subject = input(\"What subject do you want to learn about and be quizzed on? \")\n",
        "    state[\"subject\"] = subject.lower()\n",
        "    return state\n",
        "\n",
        "def generate_mcqs(state: dict) -> dict:\n",
        "    \"\"\"Generates 5 MCQ questions on the given subject and stores them in the state.\"\"\"\n",
        "    subject = state[\"subject\"]\n",
        "    prompt = (\n",
        "        f\"Generate 5 multiple-choice questions about {subject}. \"\n",
        "        \"Each question should have 4 options (A, B, C, D) and indicate the correct answer. \"\n",
        "        \"Respond in JSON format as a list of objects, where each object has keys 'question', 'options', and 'answer'.\"\n",
        "        \"Do not include any ''' formatting\"\n",
        "    )\n",
        "    # Ensure llm is defined or imported if needed here\n",
        "    # response = llm.invoke([HumanMessage(content=prompt)]) # Uncomment and ensure llm is available\n",
        "\n",
        "    # Mock response for demonstration if llm is not available\n",
        "    # response_content = \"\"\"\n",
        "    # [\n",
        "    #     {\n",
        "    #         \"question\": \"What is the chemical symbol for water?\",\n",
        "    #         \"options\": {\"A\": \"O2\", \"B\": \"H2O\", \"C\": \"CO2\", \"D\": \"NaCl\"},\n",
        "    #         \"answer\": \"B\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the capital of France?\",\n",
        "    #         \"options\": {\"A\": \"Berlin\", \"B\": \"Madrid\", \"C\": \"Paris\", \"D\": \"Rome\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the largest planet in our solar system?\",\n",
        "    #         \"options\": {\"A\": \"Earth\", \"B\": \"Mars\", \"C\": \"Jupiter\", \"D\": \"Venus\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"What is the square root of 64?\",\n",
        "    #         \"options\": {\"A\": \"6\", \"B\": \"7\", \"C\": \"8\", \"D\": \"9\"},\n",
        "    #         \"answer\": \"C\"\n",
        "    #     },\n",
        "    #     {\n",
        "    #         \"question\": \"Who wrote 'Romeo and Juliet'?\",\n",
        "    #         \"options\": {\"A\": \"Charles Dickens\", \"B\": \"William Shakespeare\", \"C\": \"Jane Austen\", \"D\": \"Leo Tolstoy\"},\n",
        "    #         \"answer\": \"B\"\n",
        "    #     }\n",
        "    # ]\n",
        "    # \"\"\"\n",
        "    # Replace the mock response with the actual LLM call when available\n",
        "    try:\n",
        "        llm=ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "        questions_json = json.loads(response.content)\n",
        "        state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON response from the language model.\")\n",
        "        print(e)\n",
        "        try:\n",
        "            cleaned_response = response.content.strip().replace(\"```json\\n\", \"\").replace(\"```\", \"\")\n",
        "            questions_json = json.loads(cleaned_response)\n",
        "            state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "        except json.JSONDecodeError as e_cleaned:\n",
        "            print(\"Error decoding JSON response after cleaning.\")\n",
        "            print(e_cleaned)\n",
        "            state[\"questions\"] = []\n",
        "\n",
        "    # Using mock response for now\n",
        "    # try:\n",
        "    #     questions_json = json.loads(response_content)\n",
        "    #     state[\"questions\"] = [(q[\"question\"], q[\"options\"], q[\"answer\"]) for q in questions_json]\n",
        "    # except json.JSONDecodeError as e:\n",
        "    #     print(\"Error decoding mock JSON response.\")\n",
        "    #     print(e)\n",
        "    #     state[\"questions\"] = []\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def main():\n",
        "    state = {}\n",
        "    state = get_subject_input(state)\n",
        "\n",
        "    while True:\n",
        "        state = generate_mcqs(state)\n",
        "\n",
        "        correct_answers = 0\n",
        "        total_questions = len(state.get(\"questions\", []))\n",
        "        answered_correctly = []\n",
        "        answered_incorrectly = []\n",
        "\n",
        "        # Display MCQs and get user answers\n",
        "        if state.get(\"questions\"):\n",
        "            print(\"\\nHere are the questions:\")\n",
        "            for i, (question, options, answer) in enumerate(state[\"questions\"]):\n",
        "                # Remove <sup> tags from the question\n",
        "                cleaned_question = re.sub(r'<sup[^>]*>.*?</sup>', '', question)\n",
        "                print(f\"\\nQuestion {i+1}: {cleaned_question}\")\n",
        "                # Iterate through the options dictionary to display letter and text\n",
        "                for letter, option_text in options.items():\n",
        "                    # Remove <sup> tags from the options\n",
        "                    cleaned_option_text = re.sub(r'<sup[^>]*>.*?</sup>', '', option_text)\n",
        "                    print(f\"{letter}. {cleaned_option_text}\")\n",
        "\n",
        "                user_answer = input(\"Your answer (enter the option letter, e.g., A): \").upper()\n",
        "\n",
        "                # Check if the answer is correct\n",
        "                # Simplified check based on the dictionary answer\n",
        "                correct_option_letter = answer\n",
        "\n",
        "\n",
        "                if user_answer == correct_option_letter:\n",
        "                    print(\"Correct!\")\n",
        "                    correct_answers += 1\n",
        "                    answered_correctly.append(question)\n",
        "                else:\n",
        "                    print(f\"Incorrect. The correct answer is {correct_option_letter}.\")\n",
        "                    answered_incorrectly.append(question)\n",
        "        else:\n",
        "            print(\"No questions were generated.\")\n",
        "\n",
        "        # Print the score\n",
        "        if total_questions > 0:\n",
        "            print(f\"\\nYour score: {correct_answers}/{total_questions}\")\n",
        "\n",
        "            # Use LLM to summarize topics\n",
        "            if answered_incorrectly:\n",
        "                print(\"\\nHere's a summary of the topics you answered incorrectly:\")\n",
        "                for incorrect_question in answered_incorrectly:\n",
        "                    incorrect_topic_prompt = f\"Explain the topic of the following question in a short paragraph:\\n{incorrect_question}\"\n",
        "                    llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash-latest\" ,temperature=0.2)\n",
        "                    incorrect_topic_response = llm.invoke([HumanMessage(content=incorrect_topic_prompt)])\n",
        "                    cleaned_incorrect_topic_explanation = incorrect_topic_response.content.replace('**', '')\n",
        "                    print(f\"\\n{cleaned_incorrect_topic_explanation}\")\n",
        "\n",
        "            # Ask for the next topic\n",
        "            next_subject = input(\"\\nWhich topic would you like to be quizzed on next? (or type 'quit' to exit) \")\n",
        "            if next_subject.lower() == 'quit':\n",
        "                break\n",
        "            state[\"subject\"] = next_subject.lower()\n",
        "            print(f\"Okay, I will generate questions about {next_subject} next.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}